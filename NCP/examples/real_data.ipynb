{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NCP.utils import frnp\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from NCP.model import NCPOperator, NCPModule\n",
    "from NCP.nn.layers import MLP\n",
    "from NCP.nn.losses import CMELoss\n",
    "from NCP.nn.nf_module import NFModule\n",
    "from NCP.utils import frnp, FastTensorDataLoader\n",
    "from NCP.cdf import get_cdf, quantile_regression, get_mean_from_nf, compute_marginal, get_cdf_from_nf\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "import numpy as np\n",
    "from NCP.metrics import smooth_cdf\n",
    "from NCP.cdf import compute_marginal\n",
    "from NCP.examples.tools.lincde import lincde\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "import normflows as nf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from uci_datasets import Dataset\n",
    "from conditionalconformal import CondConf\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "houseelectric dataset, N=2049280, d=11\n"
     ]
    }
   ],
   "source": [
    "xscaler = StandardScaler()\n",
    "yscaler = StandardScaler()\n",
    "\n",
    "data = Dataset('houseelectric')\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = data.get_split(split=0)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "# subsampling for computation reasons:\n",
    "X_train = X_train[:50000]\n",
    "Y_train = Y_train[:50000].reshape(-1, 1)\n",
    "X_val = X_val[:1000]\n",
    "Y_val = Y_val[:1000].reshape(-1, 1)\n",
    "X_test = X_test[:200]\n",
    "Y_test = Y_test[:200].reshape(-1,1)\n",
    "\n",
    "X_train = xscaler.fit_transform(X_train)\n",
    "Y_train = yscaler.fit_transform(Y_train)\n",
    "\n",
    "X_val = xscaler.transform(X_val)\n",
    "Y_val = yscaler.transform(Y_val)\n",
    "X_test = xscaler.transform(X_test)\n",
    "Y_test = yscaler.transform(Y_test)\n",
    "\n",
    "X_train_torch = frnp(X_train)\n",
    "Y_train_torch = frnp(Y_train)\n",
    "X_val_torch = frnp(X_val)\n",
    "Y_val_torch = frnp(Y_val)\n",
    "X_test_torch = frnp(X_test)\n",
    "Y_tes_torch = frnp(Y_test)\n",
    "\n",
    "#y discretisation for computing cdf\n",
    "spread = np.max(Y_train) - np.min(Y_train)\n",
    "p1, p99 = np.min(Y_train), np.max(Y_train)\n",
    "y_discr, step = np.linspace(p1-0.1*spread, p99+0.1*spread, num=5000, retstep=True)\n",
    "y_discr_torch = torch.Tensor(y_discr.reshape((-1, 1)))\n",
    "\n",
    "k_pdf = compute_marginal(bandwidth='scott').fit(Y_train)\n",
    "marginal = lambda x : torch.Tensor(np.exp(k_pdf.score_samples(x.reshape(-1, 1))))\n",
    "\n",
    "train_dl = FastTensorDataLoader(X_train_torch, Y_train_torch, batch_size=len(X_train_torch), shuffle=False)\n",
    "val_dl = FastTensorDataLoader(X_val_torch, Y_val_torch, batch_size=len(X_val_torch), shuffle=False)\n",
    "\n",
    "alpha = 0.1\n",
    "NEXP=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1237: 100%|██████████| 1/1 [00:00<00:00,  4.45it/s, v_num=130, val_loss=2.400, train_loss=-0.0698]  \n"
     ]
    }
   ],
   "source": [
    "output_shape=500\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class CustomModelCheckpoint(ModelCheckpoint):\n",
    "    def on_save_checkpoint(self, trainer, pl_module, checkpoint):\n",
    "        X, Y = trainer.model.batch\n",
    "        trainer.model.model._compute_data_statistics(X, Y)\n",
    "\n",
    "def restore_buffers_shape(model, state_dict):\n",
    "    model._sing_val = torch.zeros_like(state_dict['model._sing_val']).to('cpu')\n",
    "    model._sing_vec_l = torch.zeros_like(state_dict['model._sing_vec_l']).to('cpu')\n",
    "    model._sing_vec_r = torch.zeros_like(state_dict['model._sing_vec_r']).to('cpu')\n",
    "\n",
    "MLP_kwargs_U = {\n",
    "    'input_shape': X_train.shape[-1],\n",
    "    'output_shape': output_shape,\n",
    "    'n_hidden': 4,\n",
    "    'layer_size': 128,\n",
    "    'dropout': 0.,\n",
    "    'iterative_whitening': False,\n",
    "    'activation': torch.nn.ReLU\n",
    "}\n",
    "\n",
    "MLP_kwargs_V = {\n",
    "    'input_shape': Y_train.shape[-1],\n",
    "    'output_shape': output_shape,\n",
    "    'n_hidden': 4,\n",
    "    'layer_size':128,\n",
    "    'dropout': 0,\n",
    "    'iterative_whitening': False,\n",
    "    'activation': torch.nn.ReLU\n",
    "}\n",
    "\n",
    "loss_fn = CMELoss\n",
    "loss_kwargs = {\n",
    "    'mode': 'split',\n",
    "    'gamma': 2e-1}\n",
    "\n",
    "reg = NCPOperator(U_operator=MLP, V_operator=MLP, U_operator_kwargs=MLP_kwargs_U, V_operator_kwargs=MLP_kwargs_V)\n",
    "optimizer_kwargs = {\n",
    "        'lr': 1e-3\n",
    "        }\n",
    "NCP_module = NCPModule(\n",
    "    reg,\n",
    "    Adam,\n",
    "    optimizer_kwargs,\n",
    "    CMELoss,\n",
    "    loss_kwargs\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=300, mode=\"min\")\n",
    "checkpoint_callback = CustomModelCheckpoint(save_top_k=1, monitor=\"val_loss\", mode=\"min\")\n",
    "\n",
    "trainer = L.Trainer(**{\n",
    "    'accelerator': device,\n",
    "    'max_epochs': int(5e3),\n",
    "    'log_every_n_steps': 1,\n",
    "    'enable_progress_bar': True,\n",
    "    'devices': 1,\n",
    "    'enable_checkpointing': True,\n",
    "    'num_sanity_val_steps': 0,\n",
    "    'enable_model_summary': False,\n",
    "    }, callbacks=[early_stop, checkpoint_callback])\n",
    "\n",
    "trainer.fit(NCP_module, train_dataloaders=train_dl, val_dataloaders=val_dl)\n",
    "\n",
    "# recover best model during training\n",
    "best_model_dict = torch.load(checkpoint_callback.best_model_path)\n",
    "restore_buffers_shape(reg, best_model_dict['state_dict'])\n",
    "NCP_module.load_state_dict(best_model_dict['state_dict'])\n",
    "best_model = NCP_module.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4999: 100%|██████████| 1/1 [00:00<00:00,  4.33it/s, v_num=123, val_loss=-2.08, train_loss=-2.27]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4999: 100%|██████████| 1/1 [00:00<00:00,  4.21it/s, v_num=123, val_loss=-2.08, train_loss=-2.27]\n"
     ]
    }
   ],
   "source": [
    "best_models_nf = {}\n",
    "\n",
    "for exp in range(1):\n",
    "    L.seed_everything(exp)\n",
    "\n",
    "    base = nf.distributions.base.DiagGaussian(1)\n",
    "\n",
    "    # Define list of flows (2 flows to emulate our two MLP approach, each with more capacity than our MLPs)\n",
    "    num_flows = 2\n",
    "    latent_size = 1\n",
    "    hidden_units = 128\n",
    "    num_blocks = 3\n",
    "    flows = []\n",
    "    for i in range(num_flows):\n",
    "        flows += [nf.flows.AutoregressiveRationalQuadraticSpline(latent_size, num_blocks, hidden_units, \n",
    "                                                    num_context_channels=X_train.shape[-1])]\n",
    "        flows += [nf.flows.LULinearPermute(latent_size)]\n",
    "\n",
    "    # If the target density is not given\n",
    "    model = nf.ConditionalNormalizingFlow(base, flows)\n",
    "\n",
    "    nf_module = NFModule(model,\n",
    "        Adam,\n",
    "        optimizer_kwargs)\n",
    "\n",
    "    checkpoint_callback_vanilla = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\")\n",
    "\n",
    "    trainer = L.Trainer(**{\n",
    "        'accelerator': device,\n",
    "        'max_epochs': int(5e3),\n",
    "        'log_every_n_steps': 1,\n",
    "        'enable_progress_bar': True,\n",
    "        'devices': 1,\n",
    "        'enable_checkpointing': True,\n",
    "        'num_sanity_val_steps': 0,\n",
    "        'enable_model_summary': False,\n",
    "        }, callbacks=[checkpoint_callback_vanilla])\n",
    "\n",
    "    trainer.fit(nf_module, train_dataloaders=train_dl, val_dataloaders=val_dl)\n",
    "\n",
    "    checkpoint_callback_vanilla.best_model_path\n",
    "    best_model_dict = torch.load(checkpoint_callback_vanilla.best_model_path)\n",
    "    nf_module.load_state_dict(best_model_dict['state_dict'])\n",
    "\n",
    "    best_models_nf[exp] = nf_module.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [39:36<00:00, 11.88s/it]\n"
     ]
    }
   ],
   "source": [
    "# plots with quantiles for one model\n",
    "\n",
    "pred_test = best_model.conditional_expectation(X_test, Y_train, postprocess='whitening').reshape(-1, 1)\n",
    "nf_pred = np.zeros(X_test.shape[0])\n",
    "nf_quantiles = np.zeros((X_test.shape[0], 2))\n",
    "pred_quantiles = np.zeros((X_test.shape[0], 2))\n",
    "for i, xi in enumerate(tqdm(X_test)):\n",
    "    nf_pred[i] = get_mean_from_nf(best_models_nf[0], torch.Tensor([xi]), 1000)\n",
    "    nf_quantiles[i] = quantile_regression(best_models_nf[0], torch.Tensor([xi]),\n",
    "                                          y_discr_torch, alpha=alpha, \n",
    "                                          postprocess='whitening', marginal=marginal, \n",
    "                                          model_type='NF')\n",
    "    pred_quantiles[i] = quantile_regression(best_model, torch.Tensor([xi]),\n",
    "                                            y_discr_torch, alpha = alpha, \n",
    "                                            postprocess='whitening', marginal=marginal, \n",
    "                                            model_type='NCP')\n",
    "nf_pred= nf_pred.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from deel.puncc.regression import SplitCP\n",
    "# split conformal with random forests\n",
    "\n",
    "trained_linear_model = RandomForestRegressor().fit(X_train, Y_train.flatten())\n",
    "\n",
    "# Instanciate the split conformal wrapper for the linear model.\n",
    "# Train argument is set to False because we do not want to retrain the model\n",
    "split_cp = SplitCP(trained_linear_model, train=False)\n",
    "\n",
    "# With a calibration dataset, compute (and store) nonconformity scores\n",
    "split_cp.fit(X_fit=X_train, y_fit=Y_train.flatten(), X_calib=X_val, y_calib=Y_val.flatten())\n",
    "\n",
    "# Obtain the model's point prediction y_pred and prediction interval\n",
    "# PI = [y_pred_lower, y_pred_upper] for a target coverage of 90% (1-alpha).\n",
    "y_pred, y_pred_lower, y_pred_upper = split_cp.predict(X_test, alpha=alpha)\n",
    "quants = np.array([y_pred_lower, y_pred_upper]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n",
      "0.885\n",
      "0.855\n"
     ]
    }
   ],
   "source": [
    "print(np.mean((Y_test.flatten() >= pred_quantiles[:,0]) *(Y_test.flatten() <= pred_quantiles[:,1])))\n",
    "print(np.mean((Y_test.flatten() >= nf_quantiles[:,0]) *(Y_test.flatten() <= nf_quantiles[:,1])))\n",
    "print(np.mean((Y_test.flatten() >= quants[:,0]) *(Y_test.flatten() <= quants[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7996339404582977 0.7520310413554643\n",
      "0.11872025499338633 0.07720246278158797\n",
      "0.11789847349119817 9.53182800550177e-17\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(pred_quantiles[:,1] - pred_quantiles[:,0]), np.std(pred_quantiles[:,1] - pred_quantiles[:,0]))\n",
    "print(np.mean(nf_quantiles[:,1] - nf_quantiles[:,0]), np.std(nf_quantiles[:,1] - nf_quantiles[:,0]))\n",
    "print(np.mean(quants[:,1] - quants[:,0]), np.std(quants[:,1] - quants[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improt matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X_test, Y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koopman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
